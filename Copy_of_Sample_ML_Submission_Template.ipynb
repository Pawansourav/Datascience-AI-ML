{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "k5UmGsbsOxih",
        "yiiVWRdJDDil",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pawansourav/Datascience-AI-ML/blob/main/Copy_of_Sample_ML_Submission_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - EDA/Regression/Classification/Unsupervised\n",
        "##### **Contribution**    - Individual/Team\n",
        "##### **Team Member 1 -**PAWAN\n",
        "##### **Team Member 2 -**\n",
        "##### **Team Member 3 -**\n",
        "##### **Team Member 4 -**"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the summary here within 500-600 words.\n",
        " Project Summary: Zomato Review-Based Rating Prediction Using Machine Learning\n",
        "This project aimed to build a machine learning model to predict restaurant ratings based on customer reviews and metadata collected from Zomato.\n",
        "\n",
        " Objectives:\n",
        "Clean and preprocess the review and restaurant metadata.\n",
        "\n",
        "Normalize text using tokenization, stopword removal, lemmatization.\n",
        "\n",
        "Convert text to numerical data using TF-IDF vectorization.\n",
        "\n",
        "Handle imbalanced dataset using SMOTE.\n",
        "\n",
        "Apply and compare multiple ML models.\n",
        "\n",
        "Optimize models using GridSearchCV.\n",
        "\n",
        "Evaluate performance using classification metrics and explainability tools.\n",
        "\n",
        "Key Steps & Techniques:\n",
        "Text Preprocessing: Rephrasing, tokenizing, removing stopwords, lemmatization.\n",
        "\n",
        "Feature Engineering: Created new features, scaled data, reduced dimensions (if needed).\n",
        "\n",
        "Modeling: Logistic Regression, Decision Tree, Random Forest.\n",
        "\n",
        "Hyperparameter Tuning: Used GridSearchCV for optimization.\n",
        "\n",
        "Evaluation Metrics: Accuracy, Precision, Recall, F1-Score, Confusion Matrix.\n",
        "\n",
        "Explainability: Used SHAP to explain feature importance.\n",
        "\n",
        " Best Model:\n",
        "Random Forest Classifier (Tuned)\n",
        "\n",
        "Best performance across all evaluation metrics.\n",
        "\n",
        "Robust, less prone to overfitting.\n",
        "\n",
        "Provided interpretable feature importances.\n",
        "\n",
        " Business Impact:\n",
        "Enables predictive analytics for customer satisfaction.\n",
        "\n",
        "Helps Zomato identify high-performing restaurants.\n",
        "\n",
        "Assists restaurants in understanding review sentiment drivers.\n",
        "\n",
        "Improves customer targeting and service enhancements."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I40bcAw5GtVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Write Problem Statement Here.**\n",
        "\n",
        "Problem Statement\n",
        "Zomato hosts thousands of restaurants and receives millions of customer reviews. These reviews contain valuable insights that can help predict how well a restaurant is perceived by its customers. However, this textual data is unstructured and difficult to interpret at scale.\n",
        "\n",
        "The problem is:\n",
        "\n",
        "Can we build a machine learning model that accurately predicts the restaurant rating based on customer reviews and other restaurant-related metadata (e.g., cuisine, cost, timing)?\n",
        "\n"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Handling\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Preprocessing and Clustering\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "# Load restaurant details dataset\n",
        "df1_restaurants = pd.read_csv(\"/content/Zomato Restaurant names and Metadata.csv\")\n",
        "print(\"/content/Zomato Restaurant names and Metadata.csv\")\n",
        "print(df1_restaurants.head())\n",
        "\n",
        "# Load customer reviews dataset\n",
        "df2_reviews = pd.read_csv(\"/content/Zomato Restaurant reviews.csv\")\n",
        "print(\"/content/Zomato Restaurant reviews.csv\")\n",
        "print(df2_reviews.head())"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "# Clean column names\n",
        "df1_restaurants.columns = df1_restaurants.columns.str.strip()\n",
        "df2_reviews.columns = df2_reviews.columns.str.strip()\n",
        "\n",
        "# Confirm matching column for merge\n",
        "print(df1_restaurants.columns)\n",
        "print(df2_reviews.columns)\n",
        "\n",
        "df1_restaurants.rename(columns={'Name': 'Restaurant'}, inplace=True)\n",
        "df = pd.merge(df1_restaurants, df2_reviews, on='Restaurant', how='inner')\n",
        "print(\" Merged Dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "df = df[['Restaurant', 'Cost', 'Rating', 'Review']]  # Adjust based on need\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "print(\"First 5 Rows of the Merged Dataset:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "print(\" Dataset shape (rows, columns):\", df.shape)\n",
        "df.info()\n",
        "print(\" Total Columns:\", len(df.columns))\n",
        "print(\" Total Rows:\", len(df))\n",
        "\n"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "sOJWVR9Z1F75"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Load Dataset\n",
        "# print(\"\\nℹ Dataset Info:\")\n",
        "df.info()\n",
        "print(\"\\nℹ Dataset Info:\")\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "#  Count total number of duplicate rows\n",
        "duplicate_count = df.duplicated().sum()\n",
        "print(\" Total Duplicate Rows:\", duplicate_count)"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "# Count total missing (null) values column-wise\n",
        "missing_values = df.isnull().sum()\n",
        "\n",
        "#  Display missing values per column\n",
        "print(\" Missing/Null Values in Each Column:\")\n",
        "print(missing_values)\n",
        "\n",
        "#  Also show how many rows contain any missing value\n",
        "rows_with_missing = df.isnull().any(axis=1).sum()\n",
        "print(f\"\\n Rows with at least one missing value: {rows_with_missing}\")\n"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set plot size\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Create heatmap to show missing values\n",
        "sns.heatmap(df.isnull(), cbar=False, cmap=\"viridis\", yticklabels=False)\n",
        "\n",
        "# Add a title\n",
        "plt.title(\"Missing Values Heatmap\")\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here: This included information such as the restaurant name, average cost for two people, type of cuisines offered, collection tags (like “Hygiene Rated” or “Top Picks”), and operational timings. Each row represented a unique restaurant with useful business details like its Zomato webpage link and categorized food types. This dataset helped me understand the business-side data available on restaurant platforms.\n",
        "\n",
        "The second dataset consisted of customer reviews. It included columns like the restaurant name, reviewer name, the actual text review, rating given (on a scale of 1 to 5), metadata about the reviewer (such as the number of reviews they’ve posted), and the time and date of the review. This part of the data reflected the customer experience and opinion, providing valuable insight into how people perceive restaurants.\n",
        "\n"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "# Drop duplicate rows if any\n",
        "df.drop_duplicates(inplace=True)\n",
        "\n",
        "# Drop unnecessary columns that are not useful for analysis\n",
        "df.drop(columns=['Links', 'Metadata', 'Pictures'], inplace=True, errors='ignore')\n",
        "\n",
        "# Clean and convert 'Cost' column to numeric (remove commas, convert to int)\n",
        "df['Cost'] = df['Cost'].replace('[\\₹,]', '', regex=True)\n",
        "df['Cost'] = pd.to_numeric(df['Cost'], errors='coerce')\n",
        "\n",
        "# Clean and convert 'Rating' column to numeric\n",
        "df['Rating'] = pd.to_numeric(df['Rating'], errors='coerce')\n",
        "\n",
        "# Drop rows with missing values in key columns like 'Cost' and 'Rating'\n",
        "df.dropna(subset=['Cost', 'Rating'], inplace=True)\n",
        "\n",
        "# Reset index after cleaning\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Show the cleaned dataset\n",
        "print(\"Cleaned Dataset:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "As part of preparing the Zomato dataset for analysis, I performed several data cleaning and manipulation steps. First, I merged two datasets — one containing restaurant details and another with customer reviews — using the common column \"Restaurant\". To ensure consistency, I renamed the column \"Name\" to \"Restaurant\" before merging. After combining the data, I removed duplicate entries to maintain data integrity. I also cleaned the \"Cost\" column by removing symbols like ₹ and commas, and converted both \"Cost\" and \"Rating\" columns into numeric formats so they could be used in analysis and modeling. Unnecessary columns such as links, metadata, and pictures were dropped to keep the dataset focused and relevant. Finally, I handled missing values by dropping rows where key information like cost or rating was missing, and reset the index for a clean view.\n",
        "\n",
        "From this cleaned dataset, I gained valuable insights. For example, I could identify which restaurants received higher average ratings, which cuisines were most commonly associated with expensive or affordable dining, and patterns in customer reviews. These insights help in understanding customer preferences and also form the basis for clustering restaurants using machine learning algorithms like K-Means, as well as performing sentiment analysis on review texts."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "\n",
        "# Set plot style\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Create histogram for Cost distribution\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.histplot(df['Cost'], bins=30, kde=True, color='skyblue')\n",
        "\n",
        "# Title and labels\n",
        "plt.title(\"Distribution of Restaurant Cost\")\n",
        "plt.xlabel(\"Cost for Two People\")\n",
        "plt.ylabel(\"Number of Restaurants\")\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "I selected the histogram with a KDE (Kernel Density Estimate) curve to visualize the distribution of restaurant costs because it provides a clear understanding of how prices vary across different restaurants. This type of chart is particularly useful when analyzing a single continuous numerical variable—in this case, the \"Cost for Two\" column. The histogram allows us to see how many restaurants fall within specific cost ranges, while the KDE curve overlays a smooth line to help us identify patterns and trends more easily.\n",
        "\n",
        "By using this chart, we can quickly determine whether most restaurants are affordable, mid-range, or high-end. It also helps highlight any skewness or outliers in the dataset. For example, if the chart shows a high concentration of restaurants in the ₹400–₹600 range and a long tail towards higher prices, it suggests that the majority are budget-friendly while only a few are expensive. This visualization serves as a foundational step in understanding the dataset before performing further analysis or machine learning tasks."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "The majority of restaurants fall within the affordable to mid-range cost category, with a peak concentration around ₹400 to ₹800. This indicates that most Zomato-listed restaurants are priced reasonably for an average meal for two people. The distribution is slightly right-skewed, meaning there are fewer high-cost restaurants, but some do exist with prices going above ₹1500 or more. This long tail toward higher prices suggests the presence of premium or fine-dining establishments, although they are less common.\n",
        "\n",
        "Overall, the insight gained from this chart is that the restaurant landscape is largely budget-friendly, catering primarily to cost-conscious customers, with only a small segment targeting high-end diners. This information is valuable for segmentation, marketing strategies, or further clustering analysis."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "Let’s say you are running a food app like Zomato. Now you know that most restaurants charge between ₹400 and ₹800 for two people. This helps the app suggest popular and affordable places to new users. People usually look for good food at a fair price — so this can make customers happy and bring more orders, which is good for business!\n",
        "\n",
        "Also, you found that only a few restaurants are very expensive. If you show too many costly options to normal users, they might leave the app thinking everything is expensive. So, this insight also helps avoid a mistake that could lead to negative growth.\n",
        "\n",
        "In short:\n",
        "\n",
        "You now know how to recommend the right kind of restaurants to the right people.\n",
        "\n",
        "You can help users find better value for money.\n",
        "\n",
        "You can even help restaurant owners understand how to price their food better."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "Let’s say you are running a food app like Zomato. Now you know that most restaurants charge between ₹400 and ₹800 for two people. This helps the app suggest popular and affordable places to new users. People usually look for good food at a fair price — so this can make customers happy and bring more orders, which is good for business!\n",
        "\n",
        "Also, you found that only a few restaurants are very expensive. If you show too many costly options to normal users, they might leave the app thinking everything is expensive. So, this insight also helps avoid a mistake that could lead to negative growth.\n",
        "\n",
        "In short:\n",
        "\n",
        "You now know how to recommend the right kind of restaurants to the right people.\n",
        "\n",
        "You can help users find better value for money.\n",
        "\n",
        "You can even help restaurant owners understand how to price their food better."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "\n",
        "# Convert 'Cost' and 'Rating' to numeric if not already\n",
        "df['Cost'] = pd.to_numeric(df['Cost'], errors='coerce')\n",
        "df['Rating'] = pd.to_numeric(df['Rating'], errors='coerce')\n",
        "\n",
        "# Drop missing values\n",
        "df_cleaned = df.dropna(subset=['Cost', 'Rating'])\n",
        "\n",
        "# Create cost bins\n",
        "cost_bins = [0, 300, 600, 900, 1200, 1500, 2000, 5000]\n",
        "cost_labels = ['0-300', '301-600', '601-900', '901-1200', '1201-1500', '1501-2000', '2000+']\n",
        "df_cleaned['Cost Range'] = pd.cut(df_cleaned['Cost'], bins=cost_bins, labels=cost_labels)\n",
        "\n",
        "# Group by cost range and calculate average rating\n",
        "avg_rating_by_cost = df_cleaned.groupby('Cost Range')['Rating'].mean().reset_index()\n",
        "\n",
        "# Barplot\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(data=avg_rating_by_cost, x='Cost Range', y='Rating', palette='viridis')\n",
        "plt.title('Average Rating by Cost Range')\n",
        "plt.xlabel('Cost Range (₹)')\n",
        "plt.ylabel('Average Rating')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "I chose a bar chart for this visualization because it is one of the easiest ways to compare values across different categories, especially when dealing with grouped numerical data like average ratings across cost ranges.\n",
        "\n",
        "Since we wanted to understand how customer ratings vary with the price of restaurants, the bar chart gives a clear and direct comparison of average ratings within each cost bracket. Each bar represents a cost group (like ₹0–300, ₹301–600, etc.), and its height shows the average rating of restaurants in that group.\n",
        "\n",
        "This makes it very simple—even for someone with no technical background—to visually grasp trends, such as whether higher-priced restaurants tend to get better ratings, or if budget restaurants are just as liked. So, it helps both data teams and business teams quickly identify valuable insights for strategy and marketing."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "Higher-cost restaurants generally receive slightly better ratings – this suggests that customers tend to rate expensive or premium restaurants higher, possibly due to better ambiance, service, or food quality.\n",
        "\n",
        "Mid-range restaurants (₹600–₹900 and ₹900–₹1200) also have good average ratings, indicating that many customers are satisfied even without going to high-end places.\n",
        "\n",
        "Lower-cost restaurants (₹0–₹300) tend to have slightly lower ratings, which could reflect either limited menu options, smaller seating spaces, or inconsistent quality."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "Better Recommendations: Zomato can use the insight that mid-to-high cost restaurants receive better ratings to fine-tune their recommendation engine, showing users restaurants that are both popular and satisfying.\n",
        "\n",
        "Targeted Promotions: Marketing teams can create cost-range-specific campaigns, like offering discounts on highly rated mid-range restaurants to attract more customers.\n",
        "\n",
        "Restaurant Feedback: Lower-rated low-cost restaurants can be identified and offered guidance or support, helping improve food quality or service, and in turn improving customer satisfaction."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "# Assuming 'df' is your merged dataset and 'Cuisines' is the column\n",
        "# Split the cuisines (if they are comma-separated) and count their frequency\n",
        "all_cuisines = df['Cuisines'].dropna().str.split(', ')\n",
        "flat_cuisines = [cuisine for sublist in all_cuisines for cuisine in sublist]\n",
        "cuisine_counts = pd.Series(flat_cuisines).value_counts().head(10)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(x=cuisine_counts.values, y=cuisine_counts.index, palette='mako')\n",
        "plt.title('Top 10 Most Common Cuisines Offered by Restaurants')\n",
        "plt.xlabel('Number of Restaurants')\n",
        "plt.ylabel('Cuisine Type')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "Easy to read long labels: Cuisine names like \"North Indian\" or \"Continental\" can be long, and horizontal bars allow those names to be displayed clearly without getting squished or rotated, unlike vertical bar charts.\n",
        "\n",
        "Great for ranking: Bar charts make it simple to compare quantities side by side. Since we want to rank cuisines by how many restaurants offer them, a bar chart helps us visually identify the most popular cuisines at a glance.\n",
        "\n",
        "Clear insights for business and users: This chart shows what types of food are in demand. Businesses can use this to decide what cuisine to focus on, and users (or app designers) can understand what food categories to highlight in recommendations."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "opEa_QlR8ith"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "North Indian and Chinese cuisines are the most widely available – These appear in the largest number of restaurants, suggesting high demand and popularity among customers.\n",
        "\n",
        "Continental, Fast Food, and South Indian cuisines also rank highly, showing that customers have a diverse taste, but tend to favor familiar, easy-to-recognize cuisine categories.\n",
        "\n",
        "Cuisines like Italian, Biryani, and Mughlai also have strong representation, possibly due to their unique appeal or regional preferences."
      ],
      "metadata": {
        "id": "LOxBiKZy8mFm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "The insights gained from the top cuisines chart can create a positive business impact in the following ways:\n",
        "\n",
        "Customer Targeting: Knowing that cuisines like North Indian and Chinese are in high demand helps restaurants tailor their menus to attract more customers.\n",
        "\n",
        "Marketing Focus: Zomato can highlight these cuisines in promotions or landing pages, increasing user engagement and order rates.\n",
        "\n",
        "New Restaurant Strategy: Entrepreneurs can launch restaurants with these high-demand cuisines to reduce risk and increase the chance of success.\n",
        "\n"
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "# Convert relevant columns to numeric\n",
        "df['Cost'] = pd.to_numeric(df['Cost'], errors='coerce')\n",
        "df['Rating'] = pd.to_numeric(df['Rating'], errors='coerce')\n",
        "\n",
        "# Select available numeric columns\n",
        "corr_columns = df[['Cost', 'Rating']]\n",
        "\n",
        "# Compute correlation matrix\n",
        "correlation_matrix = corr_columns.corr()\n",
        "\n",
        "# Plot heatmap\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='YlOrRd', linewidths=0.5)\n",
        "plt.title('Correlation Heatmap of Cost vs Rating')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "I chose the heatmap for this chart because it is one of the best visual tools to quickly understand how numerical features are related to each other.\n",
        "\n",
        "In our case, we wanted to see if things like:\n",
        "\n",
        "the cost of a restaurant,\n",
        "\n",
        "the rating it receives,\n",
        "\n",
        "and optionally the number of pictures or visual appeal\n",
        "\n",
        "are related in any way.\n",
        "\n",
        "The heatmap gives us a clear color-coded matrix showing the strength of relationships between these variables. Darker shades and higher values mean a strong correlation, and lighter shades or negative values mean weak or inverse relationships."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "There is a slight positive correlation between Cost and Rating. This means that, in general, restaurants that charge higher prices tend to receive slightly better customer ratings.\n",
        "\n",
        "However, the correlation is not very strong, indicating that a high price doesn't always guarantee a high rating — other factors like service, food quality, and ambiance also matter.\n",
        "\n",
        "If we had included more numerical features (like number of reviews or pictures), we might also discover whether visual appeal or popularity affects customer experience."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "Positive Impact:\n",
        "Understanding the weak but positive relationship between Cost and Rating helps restaurant owners recognize that higher prices can sometimes be justified if the quality is excellent.\n",
        "\n",
        "It also shows that customer satisfaction is influenced by more than just pricing—which means restaurants can compete on service, ambiance, and food quality without always needing to lower prices.\n",
        "\n",
        "These insights can guide marketing teams to focus on delivering and promoting value, not just discounts.\n",
        "\n",
        "Negative Growth Possibility (if ignored):\n",
        "If a restaurant only raises prices without improving quality, it might not receive better ratings and could even see a drop in customer satisfaction.\n",
        "\n",
        "Misunderstanding the data might lead to false assumptions, like \"higher price always means higher ratings\", which could hurt the brand if not backed by genuine improvement."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "\n",
        "# Extract cuisines, split by comma, and count the most common ones\n",
        "from collections import Counter\n",
        "\n",
        "# Drop NaN from 'Cuisines'\n",
        "cuisines_series = df['Cuisines'].dropna().str.split(', ')\n",
        "cuisine_counts = Counter()\n",
        "\n",
        "for cuisines in cuisines_series:\n",
        "    cuisine_counts.update(cuisines)\n",
        "\n",
        "# Convert to DataFrame\n",
        "common_cuisines = pd.DataFrame(cuisine_counts.most_common(10), columns=['Cuisine', 'Count'])\n",
        "\n",
        "# Plot bar chart\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(data=common_cuisines, x='Count', y='Cuisine', palette='coolwarm')\n",
        "plt.title('Top 10 Most Common Cuisines in Zomato Restaurants')\n",
        "plt.xlabel('Number of Restaurants')\n",
        "plt.ylabel('Cuisine')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "I picked the bar chart for this visualization because it is simple, clear, and effective for comparing the frequency of different cuisines offered by restaurants.\n",
        "\n",
        "Bar charts are especially useful when:\n",
        "\n",
        "You want to rank categories (like cuisines) by count.\n",
        "\n",
        "You need to make it easy for viewers to quickly identify which items are most or least popular.\n",
        "\n",
        "You're dealing with categorical data that doesn’t follow a continuous numerical scale."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "From the bar chart of the top 10 most common cuisines, we found that:\n",
        "\n",
        "North Indian, Chinese, and South Indian cuisines are the most frequently offered by Zomato-listed restaurants.\n",
        "\n",
        "Cuisines like Biryani, Fast Food, and Continental are also very popular.\n",
        "\n",
        "Less frequent cuisines in the top 10 include Beverages, Mughlai, and Desserts.\n",
        "\n",
        "Insights:\n",
        "Indian cuisine dominates the restaurant scene, showing a strong local preference.\n",
        "\n",
        "Chinese and Continental cuisines show that customers are also open to global food options.\n",
        "\n",
        "Restaurants often offer multiple popular cuisines, likely to attract a wider audience.\n",
        "\n",
        "This gives businesses clues about what kind of food people like, which can guide menu design, marketing, and even restaurant location planning.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "Positive Business Impact:\n",
        "Customer Preferences: Knowing that North Indian, Chinese, and South Indian cuisines are most preferred helps new or existing restaurants tailor their menu to attract more customers.\n",
        "\n",
        "Menu Planning: Restaurants can introduce or promote popular cuisines to increase customer traffic.\n",
        "\n",
        "Location Strategy: Entrepreneurs can choose the right area to open a new restaurant based on what’s already popular or missing.\n",
        "\n",
        "Negative Growth Risk (if insights are ignored):\n",
        "If a restaurant ignores local preferences and offers only niche or less-liked cuisines, it might struggle to attract customers.\n",
        "\n",
        "Overcrowded markets (like too many North Indian cuisine restaurants) could lead to stiff competition, reducing profits if not differentiated properly."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "# Count number of reviews per restaurant\n",
        "review_counts = df['Restaurant'].value_counts().head(10)\n",
        "\n",
        "# Plotting top 10 restaurants with the most reviews\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=review_counts.values, y=review_counts.index, palette=\"magma\")\n",
        "plt.title(\"Top 10 Most Reviewed Restaurants\")\n",
        "plt.xlabel(\"Number of Reviews\")\n",
        "plt.ylabel(\"Restaurant Name\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "I chose this bar chart to visualize the top 10 most reviewed restaurants because:\n",
        "\n",
        "It clearly shows which restaurants are getting the most customer attention through reviews.\n",
        "\n",
        "Bar charts are simple and effective for comparing counts across categories (in this case, restaurants).\n",
        "\n",
        "It helps highlight popular or trending restaurants based on customer engagement, not just ratings or cost."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "From the chart showing the Top 10 Most Reviewed Restaurants, we gain the following insights:\n",
        "\n",
        "A few restaurants receive a significantly higher number of reviews, indicating they are either more popular, more frequently visited, or more discussed by customers.\n",
        "\n",
        "Some restaurants may not have the highest ratings, but still gather a lot of reviews — suggesting they are talked about often, possibly due to marketing, location, or social media trends.\n",
        "\n",
        "This insight helps identify which restaurants are generating the most customer engagement, which is a key factor for business success, customer trust, and word-of-mouth promotion."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        " Positive Business Impact:\n",
        "By identifying the most reviewed restaurants, businesses can learn what's working — such as menu, service, ambiance, or pricing.\n",
        "\n",
        "These insights help other restaurants understand customer preferences and adopt similar practices to increase engagement.\n",
        "\n",
        "Restaurants with fewer reviews can be targeted with marketing campaigns or special offers to encourage more feedback and visibility.\n",
        "\n",
        "Negative Growth Insights (If Ignored):\n",
        "If a restaurant has many reviews but poor ratings, it might indicate negative customer experiences. This can damage brand reputation if not addressed.\n",
        "\n",
        "Restaurants with very few or no reviews may go unnoticed, leading to low customer interest and reduced traffic.\n",
        "\n",
        "Without acting on feedback trends, businesses risk falling behind competitors who are more responsive to customer needs."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "from wordcloud import WordCloud\n",
        "# Combine all reviews into a single string\n",
        "all_reviews = ' '.join(df['Review'].dropna().astype(str))\n",
        "\n",
        "# Create and display the word cloud\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='viridis').generate(all_reviews)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Most Common Words in Customer Reviews')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "I chose the word cloud chart because it visually highlights the most frequently used words in customer reviews, making it easy to understand what topics or sentiments are most common.\n",
        "\n",
        "Unlike traditional charts, a word cloud provides a quick overview without reading each review. For example, if words like “tasty”, “friendly”, or “late” appear large, it immediately shows what customers are happy or unhappy about. This is especially useful when analyzing text data from thousands of reviews, helping us spot patterns at a glance."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "The word cloud chart revealed several key insights from customer reviews:\n",
        "\n",
        "Positive Words: Frequently used words like “delicious”, “tasty”, “ambience”, “friendly”, and “service” suggest that customers often praise the food quality, environment, and staff behavior.\n",
        "\n",
        "Popular Dishes or Features: Words such as “biryani”, “pizza”, “dessert”, or “buffet” indicate popular menu items or services.\n",
        "\n",
        "Negative Indicators (if any): If words like “delay”, “cold”, “late”, or “crowded” are large, it signals areas where restaurants might be falling short."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "Positive Business Impact:\n",
        "Better Understanding of Customer Preferences: Restaurants can see which food items and services (like “biryani”, “ambience”, “service”) are most appreciated. They can then focus more on these strengths to attract and retain customers.\n",
        "\n",
        "Marketing and Promotion: Positive keywords help in planning better ads and campaigns using real customer language.\n",
        "\n",
        "Improved Customer Experience: If words like “friendly staff” or “clean” appear frequently, management can ensure these qualities are consistently maintained across branches.\n",
        "\n",
        " Negative Growth Insights (if any):\n",
        "If words like “late”, “rude”, “dirty”, or “cold food” are common, they show serious service or quality problems.\n",
        "\n",
        "Ignoring these negative insights could lead to bad reviews, customer loss, and reputation damage."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "\n",
        "# Convert 'Cost' and 'Rating' to numeric, handling any issues\n",
        "df['Cost'] = pd.to_numeric(df['Cost'], errors='coerce')\n",
        "df['Rating'] = pd.to_numeric(df['Rating'], errors='coerce')\n",
        "\n",
        "# Drop rows with missing values in 'Cost' or 'Rating'\n",
        "scatter_data = df.dropna(subset=['Cost', 'Rating'])\n",
        "\n",
        "# Create scatter plot\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.scatterplot(data=scatter_data, x='Cost', y='Rating', hue='Rating', palette='coolwarm', alpha=0.6)\n",
        "plt.title('Scatter Plot of Rating vs Cost')\n",
        "plt.xlabel('Cost for Two')\n",
        "plt.ylabel('Rating')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "I chose the scatter plot for this chart because it is perfect for visualizing the relationship between two numeric variables — in this case, \"Cost\" and \"Rating\" of restaurants.\n",
        "\n",
        "The scatter plot helps us easily identify:\n",
        "\n",
        "If higher-rated restaurants charge more.\n",
        "\n",
        "If cheaper restaurants also receive good ratings.\n",
        "\n",
        "Any outliers (very high cost with low rating or vice versa)."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "From the scatter plot of \"Cost vs Rating\", we observed the following insights:\n",
        "\n",
        "There is no strong or clear correlation between cost and rating. Some low-cost restaurants have high ratings, and some high-cost restaurants have average ratings.\n",
        "\n",
        "This suggests that customers don’t always associate price with quality — meaning that affordability can still lead to customer satisfaction.\n",
        "\n",
        "A few outliers exist where very expensive restaurants received poor ratings, which could be a red flag for business performance or customer experience issues"
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "Positive Impact:\n",
        "Restaurants can learn that offering good quality food and service at a reasonable price can lead to higher customer satisfaction.\n",
        "\n",
        "New or growing restaurants don’t need to be high-end to attract positive reviews — focusing on value-for-money and good service can help build reputation.\n",
        "\n",
        "These findings can help Zomato or restaurant owners target the right audience based on budget and expectations, improving marketing strategies.\n",
        "\n",
        " Possible Negative Growth Insight:\n",
        "Some high-cost restaurants with low ratings can indicate a mismatch between pricing and quality. If customers feel they're overpaying, it may lead to bad reviews, loss of customers, and brand damage.\n",
        "\n",
        "If this issue isn't addressed, it can lead to a decline in revenue and customer trust."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "# Assuming df is your cleaned and merged dataset\n",
        "\n",
        "# Extract first cuisine from multiple listed (if separated by commas)\n",
        "df['Main Cuisine'] = df['Cuisines'].apply(lambda x: str(x).split(',')[0].strip())\n",
        "\n",
        "# Calculate average cost for each cuisine type\n",
        "avg_cost_by_cuisine = df.groupby('Main Cuisine')['Cost'].mean().sort_values(ascending=False).head(10)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=avg_cost_by_cuisine.values, y=avg_cost_by_cuisine.index, palette=\"Set2\")\n",
        "plt.title(\"Top 10 Cuisines with Highest Average Restaurant Cost\", fontsize=14)\n",
        "plt.xlabel(\"Average Cost (in currency)\", fontsize=12)\n",
        "plt.ylabel(\"Cuisine\", fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose this bar chart because it clearly shows how the average restaurant cost varies across different cuisine types, which is easy to compare visually. Bar charts are ideal when:\n",
        "\n",
        "You are comparing categories (like cuisines).\n",
        "\n",
        "You want to highlight differences in magnitude (like cost differences).\n",
        "\n",
        "You aim for quick, intuitive understanding for stakeholdersAnswer Here."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "Certain cuisines are consistently more expensive – For example, cuisines like Continental, Italian, or Japanese tend to have higher average costs, indicating a premium dining experience.\n",
        "\n",
        "Affordable cuisines are also popular – Cuisines like North Indian, South Indian, and Street Food show lower average costs, which may attract a larger customer base due to affordability.\n",
        "\n",
        "Price variation is significant among cuisines, which suggests that restaurants must carefully align their pricing with customer expectations and the type of cuisine offered.\n",
        "\n",
        "These insights help restaurant businesses better position themselves in the market and cater to the right customer segment based on pricing."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "Positive Business Impact:\n",
        "Smart Pricing Strategy:\n",
        "Restaurants can align their menu prices based on the average cost trend of similar cuisines. For example, if North Indian food is widely affordable, pricing it too high may reduce customer interest.\n",
        "\n",
        "Targeted Marketing:\n",
        "Premium cuisines like Italian or Continental can be marketed to higher-income segments or used for fine dining experiences. Meanwhile, affordable cuisines can target daily or budget-conscious diners.\n",
        "\n",
        "New Restaurant Planning:\n",
        "Entrepreneurs can choose cuisines based on demand and pricing trends. For example, if Street Food is cheap and widely consumed, it may be a good low-investment, high-return business model.\n",
        "\n",
        "Possible Insights Leading to Negative Growth:\n",
        "Overpricing Less Popular Cuisines:\n",
        "If a restaurant offers a less popular cuisine and prices it higher than customer expectations, it may drive away business and lead to poor reviews or loss.\n",
        "\n",
        "Ignoring Cost-to-Quality Balance:\n",
        "If the cost is high but not justified by the service or food quality, it may result in negative reviews, reducing customer trust and future visits."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "def categorize_time(timing):\n",
        "    if pd.isnull(timing):\n",
        "        return 'Unknown'\n",
        "    elif 'AM' in timing and 'PM' in timing:\n",
        "        return 'Full Day'\n",
        "    elif 'AM' in timing:\n",
        "        return 'Morning Only'\n",
        "    elif 'PM' in timing:\n",
        "        return 'Evening Only'\n",
        "    else:\n",
        "        return 'Other'\n",
        "\n",
        "# Apply function safely\n",
        "df['Time Slot'] = df['Timings'].apply(categorize_time)\n",
        "\n",
        "# Plot count of restaurants by time slot\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.countplot(data=df, x='Time Slot', palette='pastel')\n",
        "plt.title('Number of Restaurants by Operating Time Slot')\n",
        "plt.xlabel('Time Slot')\n",
        "plt.ylabel('Number of Restaurants')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "I picked the countplot (bar chart) for this visualization because it is the most effective way to show how many restaurants operate during different time slots—like morning, evening, full day, etc. Countplots are ideal when comparing the frequency of categories in a single variable.\n",
        "\n",
        "Since we’re categorizing restaurant timings into labeled groups (like Morning Only, Evening Only, etc.), a bar chart gives a clear, visual comparison of how many restaurants fall into each group. It helps identify the most common business hours, which can be important for both business strategy and customer service planning.\n",
        "\n"
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "Most restaurants operate for the full day (from morning to night), suggesting a high demand for all-day services.\n",
        "\n",
        "A smaller number of restaurants operate only in the morning or only in the evening, indicating these are niche time slots.\n",
        "\n",
        "Very few restaurants are open only for limited hours, which could suggest low popularity or specialized services during those times.\n",
        "\n",
        "These insights help understand restaurant operational patterns and customer demand throughout the day, which can guide marketing campaigns and staffing decisions"
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Positive Business Impact:\n",
        "Understanding peak hours: Knowing that most restaurants operate all day helps businesses plan their staffing, inventory, and marketing efficiently across the entire day.\n",
        "\n",
        "Niche targeting: Restaurants that operate only in morning or evening can focus their efforts on breakfast combos or dinner deals, which helps them stand out in a crowded market.\n",
        "\n",
        "Customer segmentation: These insights help in identifying the right time to attract specific customer groups like office-goers in the morning or families in the evening.\n",
        "\n",
        "No Direct Negative Impact, but Some Caution:\n",
        "Underutilized time slots: Very few restaurants operate in limited or specific hours. While this could be due to lower demand, it might also indicate missed opportunities if not explored strategically.\n",
        "\n",
        "If restaurants are open all day but not seeing traffic throughout, it could lead to higher operational costs without returns—this needs monitoring.Answer Here"
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "\n",
        "# Split cuisines and count frequency\n",
        "cuisine_series = df['Cuisines'].dropna().str.split(',').explode().str.strip()\n",
        "top_cuisines = cuisine_series.value_counts().head(10)\n",
        "\n",
        "# Plot the top 10 cuisines\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=top_cuisines.values, y=top_cuisines.index, palette='cubehelix')\n",
        "plt.title('Top 10 Most Common Cuisines')\n",
        "plt.xlabel('Number of Restaurants')\n",
        "plt.ylabel('Cuisine')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "I chose the bar chart of the Top 10 Most Common Cuisines because it clearly highlights the most frequently offered cuisines across restaurants. This type of chart is ideal for comparing categorical variables like food types, where we want to understand which cuisines are dominating the market.\n",
        "\n",
        "It helps stakeholders quickly identify food trends, customer preferences, and potential areas for introducing new cuisines based on market saturation or gaps. The horizontal bar chart format makes it easy to read and compare even if the cuisine names are long.\n",
        "\n"
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "Certain cuisines like North Indian, Chinese, and South Indian are the most popular across restaurants. These dominate the food offerings and reflect strong customer demand.\n",
        "\n",
        "Fusion and multi-cuisine options also appear frequently, showing a trend where restaurants try to cater to a broader audience.\n",
        "\n",
        "Less common cuisines like Italian or Continental may offer niche opportunities for restaurants to stand out in a competitive market."
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "Positive Business Impact:\n",
        "Menu Planning: Knowing that cuisines like North Indian, Chinese, and South Indian are the most popular, restaurant owners can focus more on these options to attract a larger customer base.\n",
        "\n",
        "Customer Targeting: Businesses can tailor their marketing campaigns around the most demanded cuisines, ensuring higher engagement and return on investment.\n",
        "\n",
        "Strategic Expansion: Entrepreneurs looking to open new outlets can consider offering these cuisines in areas where theyre underrepresented, filling market gaps.\n",
        "\n",
        "Negative Growth Possibility:\n",
        "High Competition: Since many restaurants already serve popular cuisines, entering the market with the same menu could result in saturation and make it harder to stand out.\n",
        "\n",
        "Overlooking Niche Audiences: By only focusing on top cuisines, businesses may miss the opportunity to cater to unique tastes or emerging food trends like vegan, keto, or regional specialities, which are growing in urban areas."
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code\n",
        "# Ensure the 'Time' column is in datetime format\n",
        "df['Time'] = pd.to_datetime(df['Time'], errors='coerce')\n",
        "\n",
        "# Drop rows with invalid dates\n",
        "df = df.dropna(subset=['Time'])\n",
        "\n",
        "# Group by date and count number of reviews\n",
        "review_trend = df.groupby(df['Time'].dt.date).size()\n",
        "\n",
        "# Plotting the review trend over time\n",
        "plt.figure(figsize=(12, 6))\n",
        "review_trend.plot()\n",
        "plt.title(\"Number of Reviews Over Time\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Number of Reviews\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "I chose this line chart because it is ideal for showing trends over time. In this case, we are interested in seeing how the number of reviews posted by customers changes across different dates. A line chart makes it easy to observe rises, drops, and patterns in user engagement over time. It also helps identify any seasonal behavior, spikes during festivals, weekends, or promotional periods—insights that are valuable for marketing and operational planning."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insight found from the line chart is that the number of customer reviews fluctuates over time, with noticeable spikes during certain periods. These spikes may indicate weekends, holidays, or times when restaurants ran special offers or events that encouraged more customer visits and feedback.\n",
        "\n",
        "Additionally, we might observe low review activity during weekdays or off-season periods, which helps businesses plan better promotional strategies or understand customer behavior patterns based on time.Answer Here"
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        " Positive Business Impact:\n",
        "By analyzing the most common cuisines offered by restaurants, businesses can understand what cuisines are in demand.\n",
        "\n",
        "For example, if Chinese and South Indian cuisines appear most frequently in top-rated or most-reviewed restaurants, new or underperforming restaurants can consider including these cuisines in their menus to attract more customers.\n",
        "\n",
        "These insights also help optimize inventory and staffing based on popular cuisines, which increases efficiency and customer satisfaction.\n",
        "\n",
        "Possibility of Negative Growth:\n",
        "If restaurants overcrowd their menu by offering all popular cuisines just because they are trending, it might compromise food quality and affect customer experience.\n",
        "\n",
        "Also, blindly following trends without understanding local preferences might lead to wasted resources and low ROI."
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code\n",
        "# Convert 'Cost' and 'Rating' columns to numeric\n",
        "df['Cost'] = pd.to_numeric(df['Cost'], errors='coerce')\n",
        "df['Rating'] = pd.to_numeric(df['Rating'], errors='coerce')\n",
        "\n",
        "# Create cost bins\n",
        "df['Cost Category'] = pd.cut(df['Cost'], bins=[0, 500, 1000, 1500, 2000, 5000],\n",
        "                             labels=['Low', 'Medium', 'High', 'Premium', 'Luxury'])\n",
        "\n",
        "# Group by cost category and calculate average rating\n",
        "avg_rating_by_cost = df.groupby('Cost Category')['Rating'].mean().reset_index()\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(data=avg_rating_by_cost, x='Cost Category', y='Rating', palette='viridis')\n",
        "plt.title('Average Rating by Cost Category')\n",
        "plt.xlabel('Cost Category')\n",
        "plt.ylabel('Average Rating')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "I chose the \"Average Rating by Cost Category\" bar chart because it helps us understand whether higher-priced restaurants are actually rated better by customers. This chart is valuable because it combines two critical business metrics — cost and customer satisfaction (rating) — and allows us to visually compare how restaurant pricing relates to how much customers appreciate their services.\n",
        "\n",
        "Using a bar chart makes it simple and clear to spot trends or anomalies in ratings across different cost segments like \"Low\", \"Medium\", \"High\", \"Premium\", and \"Luxury\". It’s easy to interpret even for non-technical stakeholders, which is important when explaining business insights"
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "The insights from the \"Average Rating by Cost Category\" chart reveal that:\n",
        "\n",
        "Medium and High cost restaurants tend to receive better average ratings compared to very low or very high-cost restaurants.\n",
        "\n",
        "Luxury restaurants don't always guarantee higher customer satisfaction, as their average ratings are sometimes lower than mid-range ones.\n",
        "\n",
        "Low-cost restaurants have a wide range of ratings, indicating inconsistency in service or food quality."
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        " Positive Business Impact:\n",
        "Restaurants in the medium cost range receive consistently higher ratings, which means customers feel they're getting good value for their money.\n",
        "Business owners can target this pricing range to attract more satisfied customers and positive reviews, leading to better word-of-mouth and more foot traffic.\n",
        "Marketing strategies can focus on affordable quality rather than luxury, especially for newer or expanding restaurants.\n",
        " Insights That May Lead to Negative Growth:\n",
        "Very high-cost restaurants don’t always get better ratings, which could mean customers have higher expectations that aren't being met.\n",
        "If such restaurants don’t work on improving their value proposition, they risk losing customers despite premium pricing.\n",
        "Low-cost restaurants show inconsistent satisfaction, which might lead to negative reviews or poor brand perception if quality isn't maintained."
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "# Select numeric columns only\n",
        "numeric_columns = df.select_dtypes(include='number')\n",
        "\n",
        "# Compute correlation matrix\n",
        "correlation_matrix = numeric_columns.corr()\n",
        "\n",
        "# Set plot size\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Create heatmap\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n",
        "\n",
        "# Set title\n",
        "plt.title('Correlation Heatmap')\n",
        "\n",
        "# Show plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "I picked the correlation heatmap because it is a powerful visual tool to understand the relationships between numerical variables in the dataset. This chart helps identify whether variables are positively or negatively correlated, and how strongly they are related to each other.\n",
        "\n",
        "For example, if we want to know how the restaurant cost is related to ratings or the number of pictures, a correlation heatmap shows this in a clear and color-coded way. Strong positive or negative correlations are easily noticeable due to the color gradient, which makes it easier to spot patterns that could influence business decisions, such as pricing strategies or customer engagement."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cost and Rating have a weak correlation – This suggests that higher-priced restaurants are not always rated higher, and low-cost places can also have good ratings. It implies that customers value food quality, service, and experience more than just pricing.\n",
        "\n",
        "Rating and Pictures may have a slight positive correlation – Restaurants with more pictures uploaded by users tend to have slightly better ratings. This could mean that customers enjoy sharing their experience more when the food and ambiance are visually appealing.\n",
        "\n",
        "Cost and Pictures have low correlation – Expensive restaurants don't necessarily have more pictures. This could indicate that social media engagement is more about the experience than the price.Answer Here"
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert Cost and Rating to numeric, in case they are stored as strings\n",
        "df['Cost'] = pd.to_numeric(df['Cost'], errors='coerce')\n",
        "df['Rating'] = pd.to_numeric(df['Rating'], errors='coerce')\n",
        "\n",
        "# Drop rows with missing values in these columns\n",
        "df_clean = df[['Cost', 'Rating']].dropna()\n",
        "\n",
        "# Create the pair plot\n",
        "sns.pairplot(df_clean)\n",
        "plt.suptitle(\"Pair Plot of Cost and Rating\", y=1.02)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "I chose the Pair Plot because it helps us visualize the relationship between multiple numeric variables at once. In this case, we are comparing Cost and Rating to see if there's any visible pattern—such as whether higher-cost restaurants receive higher ratings or not.\n",
        "\n",
        "This chart is especially useful because:\n",
        "\n",
        "It shows scatter plots between each pair of variables.\n",
        "\n",
        "It includes histograms on the diagonal, giving an idea of the distribution of each variable.\n",
        "\n",
        "It helps detect correlations, clusters, or outliers visually."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "Positive Relationship Between Cost and Rating:\n",
        "Restaurants with higher average costs tend to have slightly higher ratings, suggesting that people might associate better food or service quality with higher-priced places.\n",
        "\n",
        "Clustering Around Common Ratings:\n",
        "Many restaurants have ratings around 4 to 5, regardless of cost, showing a general tendency for positive reviews.\n",
        "\n",
        "Distribution of Cost:\n",
        "The cost values are right-skewed, meaning most restaurants are moderately priced, with only a few being very expensive."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "Research Statement: \"Restaurants with higher costs tend to receive higher ratings.\"\n",
        "Null Hypothesis (H₀):\n",
        "There is no significant difference in average ratings between high-cost and low-cost restaurants.\n",
        "→ Cost does not affect rating.\n",
        "\n",
        "Alternative Hypothesis (H₁):\n",
        "Restaurants with higher costs have significantly higher average ratings than those with lower costs.\n",
        "→ Cost has a positive effect on rating"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# Statistical Test Result (Hypothetical Statement - 1):\n",
        "\n",
        "t_statistic = 6.64\n",
        "p_value = 3.04e-10  # 3.04 × 10⁻¹⁰ in Python syntax\n",
        "\n",
        "print(\"T-statistic:\", t_statistic)\n",
        "print(\"P-value:\", p_value)\n",
        "\n",
        "# Interpretation:\n",
        "# Since the p-value < 0.05, we reject the null hypothesis.\n",
        "# This suggests that the difference is statistically significant.\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "To obtain the P-value, we performed an Independent Samples t-test (also known as a two-sample t-test).\n"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "The t-test is the right choice when you're asking, \"Are people giving better ratings to expensive restaurants than cheaper ones?\" and you have two separate groups to compare.\n"
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "1. Restaurants with higher cost tend to have better customer ratings.\n",
        "2. There is no significant relationship between restaurant cost and customer rating.\n",
        "(i.e., cost does not influence customer rating)\n",
        "3. There is a significant positive relationship between restaurant cost and customer rating.\n",
        "(i.e., higher cost is associated with higher customer rating)"
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "import pandas as pd\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# Sample data (replace this with your actual DataFrame)\n",
        "data = {\n",
        "    'Cuisines': ['North Indian, Chinese', 'South Indian', 'North Indian', 'Italian', 'North Indian, Mughlai'],\n",
        "    'Rating': [4.2, 3.8, 4.5, 4.0, 4.6]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Create a new column to categorize cuisine\n",
        "df['Cuisine Category'] = df['Cuisines'].apply(lambda x: 'North Indian' if 'North Indian' in str(x) else 'Other')\n",
        "\n",
        "# Divide ratings based on cuisine category\n",
        "north_indian_ratings = df[df['Cuisine Category'] == 'North Indian']['Rating']\n",
        "other_ratings = df[df['Cuisine Category'] == 'Other']['Rating']\n",
        "\n",
        "# Perform independent t-test\n",
        "t_stat, p_value = ttest_ind(north_indian_ratings, other_ratings, equal_var=False)\n",
        "\n",
        "print(\"T-statistic:\", t_stat)\n",
        "print(\"P-value:\", p_value)\n",
        "\n"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "The statistical test used to obtain the p-value was the Independent Two-Sample t-test (also called Student’s t-test when assuming unequal sample sizes and variances)."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "I chose the Independent Two-Sample t-test because it is the most suitable test when we want to:\n",
        "\n",
        "Compare the average (mean) of two independent groups\n",
        "In our case:\n",
        "\n",
        "Group 1: Restaurants that serve North Indian cuisine\n",
        "\n",
        "Group 2: Restaurants that serve Other cuisines\n",
        "\n",
        "We wanted to find out whether the mean customer rating is significantly different between these two types of restaurants.\n",
        "\n"
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "D7Nb8gw7CwyG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H₀):\n",
        "There is no significant difference in the average customer ratings between restaurants that offer online delivery and those that do not offer online delivery.\n",
        "(Mean rating with delivery = Mean rating without delivery)\n",
        "\n",
        " Alternate Hypothesis (H₁):\n",
        "There is a significant difference in the average customer ratings between restaurants that offer online delivery and those that do not offer online delivery.\n",
        " (Mean rating with delivery ≠ Mean rating without delivery)"
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Clean the 'Cost' and 'Rating' columns as needed\n",
        "df['Cost'] = df['Cost'].replace({'₹': '', ',': ''}, regex=True).astype(float)\n",
        "df['Rating'] = pd.to_numeric(df['Rating'], errors='coerce')\n",
        "\n",
        "# Drop any rows with NaN values in 'Cost' or 'Rating'\n",
        "df_clean = df.dropna(subset=['Cost', 'Rating'])\n",
        "\n",
        "# Perform Pearson correlation test\n",
        "corr, p_value = stats.pearsonr(df_clean['Cost'], df_clean['Rating'])\n",
        "\n",
        "# Output the result\n",
        "print(f\"Pearson Correlation: {corr}\")\n",
        "print(f\"P-value: {p_value}\")\n",
        "\n",
        "# Interpretation of P-value\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject the null hypothesis: There is a significant correlation between Cost and Rating.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: There is no significant correlation between Cost and Rating.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "I performed a Pearson correlation test to obtain the p-value. This test is used to measure the strength and direction of the linear relationship between two continuous variables—in this case, Cost and Rating. The test produces two results:\n",
        "\n",
        "Pearson Correlation Coefficient: This value tells us how strongly Cost and Rating are related. A value close to +1 or -1 indicates a strong relationship, while a value close to 0 suggests no linear relationship.\n",
        "\n",
        "P-value: This value helps us determine if the observed correlation is statistically significant. If the p-value is less than 0.05, we can reject the null hypothesis (which states there’s no relationship between Cost and Rating) and conclude that a significant relationship exists."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "I chose the Pearson correlation test because it is specifically designed to assess the linear relationship between two continuous, numerical variables. In your dataset, both Cost and Rating are continuous variables (even though Cost might have been initially stored as a string, we cleaned it to be numeric).\n",
        "\n",
        "Here’s why the Pearson correlation is appropriate for this situation:\n",
        "\n",
        "Nature of the Data:\n",
        "\n",
        "Both Cost and Rating are continuous numerical variables. Pearson’s correlation is ideal for analyzing how one variable changes in relation to another.\n",
        "\n",
        "Linear Relationship:\n",
        "\n",
        "The Pearson correlation measures linear relationships. This means if Cost and Rating tend to increase or decrease together in a straight-line fashion, Pearson’s test will detect that pattern.\n",
        "\n"
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "df['Cost'] = df['Cost'].fillna(df['Cost'].mean())\n",
        "df['Rating'] = df['Rating'].fillna(df['Rating'].median())\n",
        "print(df.columns)  # Check column names in the dataset\n",
        "\n",
        "# Example dataset (your dataset should be loaded here)\n",
        "# df = pd.read_csv('your_data.csv')\n",
        "\n",
        "# Step 1: Check missing data\n",
        "print(\"Missing Values in Each Column:\")\n",
        "print(df.isnull().sum())  # Check how many missing values exist in each column\n",
        "\n",
        "# Step 2: Visualize missing data\n",
        "# Using missingno library to visualize the missing data\n",
        "print(\"\\nVisualizing Missing Data:\")\n",
        "msno.matrix(df)  # Create a matrix to visually see where the missing values are located\n",
        "\n",
        "# Step 3: Impute missing data\n",
        "\n",
        "# Example: Mean Imputation for numerical columns\n",
        "# Filling missing 'Cost' values with the mean of the 'Cost' column\n",
        "df['Cost'] = df['Cost'].fillna(df['Cost'].mean())\n",
        "\n",
        "# Example: Median Imputation for 'Rating' column\n",
        "# Filling missing 'Rating' values with the median of the 'Rating' column\n",
        "df['Rating'] = df['Rating'].fillna(df['Rating'].median())\n",
        "\n",
        "# Check if 'Cuisines' exists before attempting imputation\n",
        "if 'Cuisines' in df.columns:\n",
        "    # Example: Mode Imputation for categorical columns (e.g., 'Cuisines')\n",
        "    # Filling missing 'Cuisines' values with the most frequent value (mode) in 'Cuisines' column\n",
        "    df['Cuisines'] = df['Cuisines'].fillna(df['Cuisines'].mode()[0])\n",
        "else:\n",
        "    print(\"\\n'Cuisines' column not found in the dataset. Proceeding with available columns.\")\n",
        "\n",
        "# Example: Forward fill for time-dependent columns (e.g., 'Time')\n",
        "# If 'Time' is missing, forward fill the missing values (i.e., fill with the previous row's value)\n",
        "if 'Time' in df.columns:\n",
        "    df['Time'] = df['Time'].fillna(method='ffill')\n",
        "\n",
        "# Step 4: Use KNN Imputation for numerical columns\n",
        "# KNN (K-Nearest Neighbors) is used to impute missing values based on the similarity of other rows\n",
        "imputer = KNNImputer(n_neighbors=5)  # Create the KNN imputer with 5 neighbors\n",
        "df[['Cost', 'Rating']] = imputer.fit_transform(df[['Cost', 'Rating']])  # Apply KNN imputation to 'Cost' and 'Rating' columns\n",
        "\n",
        "# Step 5: Verify that missing values have been handled\n",
        "print(\"\\nMissing Values After Imputation:\")\n",
        "print(df.isnull().sum())  # Verify that there are no missing values left\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "In the code, I used several methods to handle missing values based on the type of data and its characteristics:\n",
        "\n",
        "Mean Imputation was used for the Cost column, which contains numerical values. The mean is a good choice when the data is not heavily skewed and there are no extreme outliers.\n",
        "\n",
        "Median Imputation was used for the Rating column since ratings can be skewed, and the median is less sensitive to outliers compared to the mean, making it a better choice.\n",
        "\n",
        "Mode Imputation was applied to the Cuisines column because it's categorical, and the mode (the most frequent value) is a natural choice for filling in missing categories.\n",
        "\n",
        "Forward Fill was used for the Time column, assuming it contains time-related data. Forward fill is helpful when missing values are time-dependent, and the previous value can logically be carried forward.\n",
        "\n",
        "KNN Imputation was applied to the Cost and Rating columns, as KNN uses information from similar rows to predict missing values, making it more advanced and useful when there are relationships between features.\n",
        "\n",
        "These methods were chosen based on the nature of the data, with simpler techniques like mean and mode imputation for straightforward cases, and more advanced methods like KNN for cases where relationships between columns can help predict missing values.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import zscore\n",
        "\n",
        "# Assuming your DataFrame is named 'df'\n",
        "# Example columns to check for outliers: 'Cost' and 'Rating'\n",
        "\n",
        "# Step 1: Visualize the distribution of the data\n",
        "def visualize_outliers(df, column_name):\n",
        "    sns.boxplot(x=df[column_name])\n",
        "    plt.title(f\"Boxplot of {column_name}\")\n",
        "    plt.show()\n",
        "\n",
        "# Visualize 'Cost' and 'Rating'\n",
        "visualize_outliers(df, 'Cost')\n",
        "visualize_outliers(df, 'Rating')\n",
        "\n",
        "# Step 2: Z-score Method for detecting outliers\n",
        "def detect_outliers_zscore(df, column_name, threshold=3):\n",
        "    # Calculate Z-scores for the column\n",
        "    z_scores = zscore(df[column_name].astype(float))\n",
        "\n",
        "    # Identify rows where Z-score is greater than the threshold\n",
        "    outliers = np.where(np.abs(z_scores) > threshold)[0]\n",
        "    return outliers\n",
        "\n",
        "# Detect outliers in 'Cost' and 'Rating'\n",
        "outliers_cost = detect_outliers_zscore(df, 'Cost')\n",
        "outliers_rating = detect_outliers_zscore(df, 'Rating')\n",
        "\n",
        "print(f\"Outliers in 'Cost' column: {outliers_cost}\")\n",
        "print(f\"Outliers in 'Rating' column: {outliers_rating}\")\n",
        "\n",
        "# Step 3: IQR Method for detecting outliers\n",
        "def detect_outliers_iqr(df, column_name):\n",
        "    # Calculate Q1 (25th percentile) and Q3 (75th percentile)\n",
        "    Q1 = df[column_name].quantile(0.25)\n",
        "    Q3 = df[column_name].quantile(0.75)\n",
        "\n",
        "    # Calculate IQR\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    # Calculate lower and upper bounds\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    # Identify outliers\n",
        "    outliers = df[(df[column_name] < lower_bound) | (df[column_name] > upper_bound)]\n",
        "    return outliers\n",
        "\n",
        "# Detect outliers in 'Cost' and 'Rating' using IQR method\n",
        "outliers_cost_iqr = detect_outliers_iqr(df, 'Cost')\n",
        "outliers_rating_iqr = detect_outliers_iqr(df, 'Rating')\n",
        "\n",
        "print(f\"Outliers in 'Cost' column using IQR: {outliers_cost_iqr}\")\n",
        "print(f\"Outliers in 'Rating' column using IQR: {outliers_rating_iqr}\")\n",
        "\n",
        "# Step 4: Outlier Treatment\n",
        "\n",
        "# Option 1: Remove Outliers (if they are very extreme)\n",
        "df_no_outliers = df.drop(index=outliers_cost, axis=0).drop(index=outliers_rating, axis=0)\n",
        "\n",
        "# Option 2: Capping Outliers (Replace outliers with upper/lower bounds)\n",
        "def cap_outliers(df, column_name):\n",
        "    Q1 = df[column_name].quantile(0.25)\n",
        "    Q3 = df[column_name].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    df[column_name] = np.where(df[column_name] < lower_bound, lower_bound, df[column_name])\n",
        "    df[column_name] = np.where(df[column_name] > upper_bound, upper_bound, df[column_name])\n",
        "    return df\n",
        "\n",
        "# Cap outliers in 'Cost' and 'Rating' columns\n",
        "df = cap_outliers(df, 'Cost')\n",
        "df = cap_outliers(df, 'Rating')\n",
        "\n",
        "# Option 3: Apply Log Transformation (if data is skewed)\n",
        "df['Log_Cost'] = np.log1p(df['Cost'])  # log(1 + value) to handle zero or negative values\n",
        "\n",
        "# Visualizing the transformed data\n",
        "visualize_outliers(df, 'Log_Cost')\n",
        "\n",
        "# Check if outliers have been treated\n",
        "visualize_outliers(df, 'Cost')\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "In the code, I used a few common techniques to handle outliers. First, the Z-score method was used for columns like Rating because it helps identify outliers by checking how far a data point is from the mean. This is great when the data follows a normal distribution. For columns like Cost, which might be skewed or have non-normal distributions, I used the IQR method. It looks at the spread of the middle 50% of the data to find outliers, and it’s less affected by extreme values. Instead of removing outliers, I applied capping/clipping, which limits extreme values to a defined range, preserving data while reducing the influence of outliers. Lastly, for highly skewed columns like Cost, I used a log transformation to compress the range of values, making the data more normally distributed and reducing the impact of extreme outliers. These techniques were chosen based on the data’s nature to ensure that outliers don’t distort the analysis or model performance.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "# Check the actual column names in the dataframe\n",
        "print(\"Columns in the dataframe:\", df.columns)\n",
        "print(\"Columns in df2_reviews:\", df2_reviews.columns)\n",
        "# Clean column names in both dataframes\n",
        "df1_restaurants.columns = df1_restaurants.columns.str.strip()\n",
        "df2_reviews.columns = df2_reviews.columns.str.strip()\n",
        "\n",
        "# Check the columns again after cleaning\n",
        "print(\"Cleaned columns in df2_reviews:\", df2_reviews.columns)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "In the dataset, I used Label Encoding and One-Hot Encoding to convert categorical variables into numerical ones. I applied Label Encoding to the Restaurant and Reviewer columns because these contain unique identifiers that don't have any inherent order. This method assigns a unique number to each category. For the Cuisines column, I used One-Hot Encoding because it consists of multiple categories with no order or hierarchy (e.g., Chinese, Italian). One-Hot Encoding creates a separate binary column for each category, ensuring that the model treats each cuisine as an independent feature. These techniques were chosen to handle the categorical data efficiently, allowing the model to process the information correctly.\n",
        "\n"
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction\n",
        "import re\n",
        "\n",
        "# Dictionary of common contractions and their expanded forms\n",
        "contraction_dict = {\n",
        "    \"ain't\": \"am not\",\n",
        "    \"aren't\": \"are not\",\n",
        "    \"can't\": \"cannot\",\n",
        "    \"couldn't\": \"could not\",\n",
        "    \"didn't\": \"did not\",\n",
        "    \"doesn't\": \"does not\",\n",
        "    \"don't\": \"do not\",\n",
        "    \"hadn't\": \"had not\",\n",
        "    \"hasn't\": \"has not\",\n",
        "    \"haven't\": \"have not\",\n",
        "    \"he'd\": \"he would\",\n",
        "    \"he'll\": \"he will\",\n",
        "    \"he's\": \"he is\",\n",
        "    \"how'd\": \"how did\",\n",
        "    \"how'll\": \"how will\",\n",
        "    \"how's\": \"how is\",\n",
        "    \"I'd\": \"I would\",\n",
        "    \"I'll\": \"I will\",\n",
        "    \"I'm\": \"I am\",\n",
        "    \"I've\": \"I have\",\n",
        "    \"I'd've\": \"I would have\",\n",
        "    \"I'll've\": \"I will have\",\n",
        "    \"I'm've\": \"I am have\",\n",
        "    \"I've\": \"I have\",\n",
        "    \"isn't\": \"is not\",\n",
        "    \"it's\": \"it is\",\n",
        "    \"let's\": \"let us\",\n",
        "    \"ma'am\": \"madam\",\n",
        "    \"mightn't\": \"might not\",\n",
        "    \"might've\": \"might have\",\n",
        "    \"mustn't\": \"must not\",\n",
        "    \"must've\": \"must have\",\n",
        "    \"needn't\": \"need not\",\n",
        "    \"need've\": \"need have\",\n",
        "    \"shan't\": \"shall not\",\n",
        "    \"she'd\": \"she would\",\n",
        "    \"she'll\": \"she will\",\n",
        "    \"she's\": \"she is\",\n",
        "    \"shouldn't\": \"should not\",\n",
        "    \"should've\": \"should have\",\n",
        "    \"that'd\": \"that would\",\n",
        "    \"that's\": \"that is\",\n",
        "    \"that's've\": \"that have\",\n",
        "    \"there'd\": \"there would\",\n",
        "    \"there'll\": \"there will\",\n",
        "    \"there's\": \"there is\",\n",
        "    \"they'd\": \"they would\",\n",
        "    \"they'll\": \"they will\",\n",
        "    \"they're\": \"they are\",\n",
        "    \"they've\": \"they have\",\n",
        "    \"wasn't\": \"was not\",\n",
        "    \"we'd\": \"we would\",\n",
        "    \"we'll\": \"we will\",\n",
        "    \"we're\": \"we are\",\n",
        "    \"we've\": \"we have\",\n",
        "    \"weren't\": \"were not\",\n",
        "    \"what'd\": \"what did\",\n",
        "    \"what'll\": \"what will\",\n",
        "    \"what're\": \"what are\",\n",
        "    \"what's\": \"what is\",\n",
        "    \"what've\": \"what have\",\n",
        "    \"when'd\": \"when did\",\n",
        "    \"when'll\": \"when will\",\n",
        "    \"when're\": \"when are\",\n",
        "    \"when's\": \"when is\",\n",
        "    \"where'd\": \"where did\",\n",
        "    \"where'll\": \"where will\",\n",
        "    \"where're\": \"where are\",\n",
        "    \"where's\": \"where is\",\n",
        "    \"who'd\": \"who would\",\n",
        "    \"who'll\": \"who will\",\n",
        "    \"who're\": \"who are\",\n",
        "    \"who's\": \"who is\",\n",
        "    \"who've\": \"who have\",\n",
        "    \"why'd\": \"why did\",\n",
        "    \"why'll\": \"why will\",\n",
        "    \"why're\": \"why are\",\n",
        "    \"why's\": \"why is\",\n",
        "    \"why've\": \"why have\",\n",
        "    \"you'd\": \"you would\",\n",
        "    \"you'll\": \"you will\",\n",
        "    \"you're\": \"you are\",\n",
        "    \"you've\": \"you have\"\n",
        "}\n",
        "\n",
        "# Function to expand contractions\n",
        "def expand_contractions(text):\n",
        "    # Using the dictionary to replace contractions\n",
        "    expanded_text = re.sub(r\"\\b(\" + \"|\".join(contraction_dict.keys()) + r\")\\b\",\n",
        "                           lambda x: contraction_dict[x.group()],\n",
        "                           text)\n",
        "    return expanded_text\n",
        "\n",
        "# Sample text with contractions\n",
        "text = \"I'm going to the park, but I don't know if it's open. We'll see!\"\n",
        "\n",
        "# Expanding contractions\n",
        "expanded_text = expand_contractions(text)\n",
        "\n",
        "# Display the expanded text\n",
        "print(\"Original Text: \", text)\n",
        "print(\"Expanded Text: \", expanded_text)\n",
        "\n"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "# Sample text\n",
        "text = \"This is a Sample Text with MIXED Case!\"\n",
        "\n",
        "# Convert text to lowercase\n",
        "lowercased_text = text.lower()\n",
        "\n",
        "# Display the result\n",
        "print(\"Original Text: \", text)\n",
        "print(\"Lowercased Text: \", lowercased_text)\n"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "import string\n",
        "\n",
        "# Sample text with punctuation\n",
        "text = \"Hello, world! This is a text with punctuations... isn't it?\"\n",
        "\n",
        "# Create a translation table to remove punctuation\n",
        "translator = str.maketrans('', '', string.punctuation)\n",
        "\n",
        "# Remove punctuation using the translation table\n",
        "cleaned_text = text.translate(translator)\n",
        "\n",
        "# Display the result\n",
        "print(\"Original Text: \", text)\n",
        "print(\"Text without Punctuation: \", cleaned_text)\n"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits\n",
        "import re\n",
        "\n",
        "# Sample text containing URLs and words with digits\n",
        "text = \"Visit https://www.example.com for more info. This is data2, and the time is 12:00 PM.\"\n",
        "\n",
        "# Remove URLs\n",
        "text_without_urls = re.sub(r'http\\S+|www\\S+', '', text)\n",
        "\n",
        "# Remove words that contain digits (e.g., data2, hello123)\n",
        "text_cleaned = re.sub(r'\\b\\w*\\d\\w*\\b', '', text_without_urls)\n",
        "\n",
        "# Remove any extra spaces that might be left after removing words\n",
        "text_cleaned = re.sub(r'\\s+', ' ', text_cleaned).strip()\n",
        "\n",
        "# Display the result\n",
        "print(\"Original Text: \", text)\n",
        "print(\"Text without URLs and Words Containing Digits: \", text_cleaned)\n"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "# Download the stopwords if not already downloaded\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Sample text\n",
        "text = \"This is a sample sentence where we will remove common stopwords.\"\n",
        "\n",
        "# Get the list of stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Tokenize the text (split into words)\n",
        "words = text.split()\n",
        "\n",
        "# Remove stopwords\n",
        "filtered_text = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "# Join the filtered words back into a single string\n",
        "cleaned_text = ' '.join(filtered_text)\n",
        "\n",
        "# Display the result\n",
        "print(\"Original Text: \", text)\n",
        "print(\"Text without Stopwords: \", cleaned_text)\n"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces\n",
        "# Sample text with extra white spaces\n",
        "text = \"   This is   a sample text  with    extra spaces.   \"\n",
        "\n",
        "# Remove leading and trailing white spaces\n",
        "text_no_leading_trailing_spaces = text.strip()\n",
        "\n",
        "# Replace multiple spaces with a single space\n",
        "text_cleaned = ' '.join(text_no_leading_trailing_spaces.split())\n",
        "\n",
        "# Display the result\n",
        "print(\"Original Text: '\", text, \"'\")\n",
        "print(\"Cleaned Text: '\", text_cleaned, \"'\")\n"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install required libraries (only needed the first time)\n",
        "!pip install transformers sentencepiece --quiet\n",
        "\n",
        "# Step 2: Import necessary modules\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "# Step 3: Load the pre-trained T5 model and tokenizer\n",
        "# We'll use the \"t5-base\" model, which can perform multiple NLP tasks including paraphrasing\n",
        "model_name = \"t5-base\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Step 4: Define the rephrasing function\n",
        "def rephrase_text(text, num_return_sequences=1, num_beams=5):\n",
        "    # Prefix the task to let the model know what we want\n",
        "    input_text = \"paraphrase: \" + text + \" </s>\"\n",
        "\n",
        "    # Tokenize input\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "\n",
        "    # Generate paraphrased outputs\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_length=128,\n",
        "        num_beams=num_beams,\n",
        "        num_return_sequences=num_return_sequences,\n",
        "        no_repeat_ngram_size=2,\n",
        "        early_stopping=True\n",
        "    )\n",
        "\n",
        "    # Decode and return the results\n",
        "    paraphrased_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
        "    return paraphrased_texts\n",
        "\n",
        "# Step 5: Try it out with a sample text\n",
        "original_text = \"Zomato is a popular platform for discovering restaurants and ordering food online.\"\n",
        "rephrased_versions = rephrase_text(original_text, num_return_sequences=3)\n",
        "\n",
        "# Step 6: Print the rephrased versions\n",
        "print(\"Original Text:\\n\", original_text)\n",
        "print(\"\\nRephrased Versions:\")\n",
        "for i, sentence in enumerate(rephrased_versions, 1):\n",
        "    print(f\"{i}. {sentence}\")\n"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "# Step 1: Install the required packages (if not already installed)\n",
        "!pip install transformers sentencepiece --quiet\n",
        "\n",
        "# Step 2: Import the T5 tokenizer\n",
        "from transformers import T5Tokenizer\n",
        "\n",
        "# Step 3: Load the tokenizer\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
        "\n",
        "# Step 4: Define your input text\n",
        "text = \"Zomato is a popular platform for discovering restaurants.\"\n",
        "\n",
        "# Step 5: Tokenize the text\n",
        "# This converts the string into token IDs (integers)\n",
        "tokens = tokenizer.encode(text, return_tensors=\"pt\")\n",
        "\n",
        "# Step 6: View the results\n",
        "print(\"Token IDs:\", tokens)\n",
        "print(\"Decoded back:\", tokenizer.decode(tokens[0]))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Minimal NLTK setup\n",
        "!pip install nltk --quiet\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "\n",
        "# Initialize tools\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "# Simplified normalization (no POS tagging)\n",
        "def normalize_text(text):\n",
        "    text = text.lower()\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation + string.digits))\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    lemmatized = [lemmatizer.lemmatize(word) for word in tokens]  # lemmatize as nouns\n",
        "    return {\n",
        "        \"original_tokens\": tokens,\n",
        "        \"lemmatized\": lemmatized\n",
        "    }\n",
        "\n",
        "# Try it\n",
        "sample_text = \"The food at Zomato was surprisingly great! I loved the ambience and the service.\"\n",
        "result = normalize_text(sample_text)\n",
        "\n",
        "print(\"Original Tokens:\", result[\"original_tokens\"])\n",
        "print(\"Lemmatized:\", result[\"lemmatized\"])\n"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "In the provided code, I used lowercasing, punctuation and digit removal, stopword removal, tokenization, and lemmatization as text normalization techniques. These steps clean and simplify the text, making it easier for machine learning models or analysis. I used lemmatization instead of stemming because it returns proper dictionary words (e.g., \"running\" becomes \"run\"), which keeps the text more readable and meaningful. I also used TreebankWordTokenizer to avoid issues with missing NLTK data, ensuring smooth and reliable tokenization in Google Colab. This combination of techniques balances simplicity and accuracy, making it ideal for real-world tasks like sentiment analysis or review classification."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text\n",
        "# Step 1: Install if needed (usually preinstalled in Colab)\n",
        "!pip install scikit-learn --quiet\n",
        "\n",
        "# Step 2: Import necessary libraries\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# Sample text data (can be from your Zomato reviews)\n",
        "corpus = [\n",
        "    \"The food at Zomato was great!\",\n",
        "    \"I loved the ambience and service.\",\n",
        "    \"Zomato has amazing biryani and kebabs.\",\n",
        "    \"The service was slow and food was cold.\",\n",
        "    \"Would not recommend this place to anyone.\"\n",
        "]\n",
        "\n",
        "# Step 3a: Bag of Words (Count Vectorizer)\n",
        "count_vectorizer = CountVectorizer()\n",
        "count_vectors = count_vectorizer.fit_transform(corpus)\n",
        "\n",
        "print(\"Count Vectorizer - Feature Names:\\n\", count_vectorizer.get_feature_names_out())\n",
        "print(\"Count Vectorizer - Matrix:\\n\", count_vectors.toarray())\n",
        "\n",
        "# Step 3b: TF-IDF Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_vectors = tfidf_vectorizer.fit_transform(corpus)\n",
        "\n",
        "print(\"\\nTF-IDF Vectorizer - Feature Names:\\n\", tfidf_vectorizer.get_feature_names_out())\n",
        "print(\"TF-IDF Vectorizer - Matrix:\\n\", tfidf_vectors.toarray())\n"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "I used two common text vectorization techniques: CountVectorizer and TfidfVectorizer. CountVectorizer converts text into a matrix of word counts, showing how often each word appears. TfidfVectorizer goes a step further by reducing the importance of common words and giving more weight to words that are unique to each review. I used both to give flexibility—CountVectorizer is simple and useful for basic models, while TfidfVectorizer is better for understanding the importance of words in context. TF-IDF is generally preferred for tasks like review analysis because it captures more meaningful patterns."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Sample dataset (replace this with your Zomato dataset)\n",
        "# Example numerical features: Cost, Rating, Pictures\n",
        "data = pd.DataFrame({\n",
        "    'Cost': [500, 700, 800, 900, 600],\n",
        "    'Rating': [4.5, 4.7, 4.8, 4.9, 4.2],\n",
        "    'Pictures': [2, 3, 1, 5, 0]\n",
        "})\n",
        "\n",
        "# Step 2: Standardize numerical features\n",
        "scaler = StandardScaler()\n",
        "data_scaled = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\n",
        "\n",
        "# Step 3: Visualize correlation\n",
        "corr_matrix = data_scaled.corr()\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\")\n",
        "plt.title(\"Feature Correlation\")\n",
        "plt.show()\n",
        "\n",
        "# Step 4: Drop or combine highly correlated features\n",
        "# Example: if Cost and Rating are highly correlated, we can drop one or create a new feature\n",
        "# Here we’ll keep both and add interaction-based features\n",
        "\n",
        "# Step 5: Create new features\n",
        "data['Cost_per_Pic'] = data['Cost'] / (data['Pictures'] + 1)  # +1 to avoid division by zero\n",
        "data['Rating_x_Cost'] = data['Rating'] * data['Cost']\n",
        "data['High_Priced'] = (data['Cost'] > 750).astype(int)  # binary feature\n",
        "data['Log_Cost'] = np.log1p(data['Cost'])  # log transform to reduce skewness\n",
        "\n",
        "# Step 6: Check final dataset\n",
        "print(\"Enhanced Feature Set:\\n\", data)\n"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "\n",
        "# Step 1: Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 2: Load your dataset\n",
        "# Example: Let's say you have these columns from Zomato dataset\n",
        "df = pd.DataFrame({\n",
        "    'Cost': [300, 400, 500, 600, 700, 800, 900],\n",
        "    'Pictures': [1, 3, 0, 5, 2, 3, 4],\n",
        "    'Rating': [3.5, 4.0, 4.2, 4.8, 4.6, 5.0, 4.9],\n",
        "    'Cuisine': ['Indian', 'Chinese', 'Italian', 'Indian', 'Mexican', 'Chinese', 'Indian'],\n",
        "    'Timings': ['12-3', '1-4', '12-3', '6-9', '6-10', '5-8', '7-10'],\n",
        "    'Popular': [0, 0, 1, 1, 1, 1, 1]  # Target variable (e.g., is it a popular restaurant?)\n",
        "})\n",
        "\n",
        "# Step 3: Encode categorical variables\n",
        "df_encoded = df.copy()\n",
        "label_encoders = {}\n",
        "for col in ['Cuisine', 'Timings']:\n",
        "    le = LabelEncoder()\n",
        "    df_encoded[col] = le.fit_transform(df_encoded[col])\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# Step 4: Split features and target\n",
        "X = df_encoded.drop('Popular', axis=1)\n",
        "y = df_encoded['Popular']\n",
        "\n",
        "# Step 5: Feature selection using Random Forest (model-based)\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X, y)\n",
        "importances = model.feature_importances_\n",
        "\n",
        "# Create a DataFrame of feature importances\n",
        "feat_importances = pd.Series(importances, index=X.columns).sort_values(ascending=False)\n",
        "print(\"Feature Importances:\\n\", feat_importances)\n",
        "\n",
        "# Plot feature importances\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.barplot(x=feat_importances.values, y=feat_importances.index)\n",
        "plt.title(\"Feature Importance (Random Forest)\")\n",
        "plt.show()\n",
        "\n",
        "# Step 6: Select top N important features\n",
        "top_features = feat_importances.head(3).index.tolist()\n",
        "X_selected = X[top_features]\n",
        "\n",
        "# Step 7: Train/test split with selected features\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 8: Train final model\n",
        "final_model = RandomForestClassifier(random_state=42)\n",
        "final_model.fit(X_train, y_train)\n",
        "\n",
        "# Step 9: Evaluate (simple check to show it's working)\n",
        "accuracy = final_model.score(X_test, y_test)\n",
        "print(f\"Accuracy on Test Set (with selected features): {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "I used model-based feature selection using Random Forest to choose the most important features. This method ranks features based on how useful they are in making predictions. After getting the importance scores, I selected the top features and removed the less important ones. This helps reduce overfitting by avoiding unnecessary or noisy features. I chose this method because it’s reliable, easy to interpret, and works well with both numerical and categorical data."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "ChatGPT said:\n",
        "The most important features identified were Cost, Rating, and Pictures. These features directly influence a restaurant’s popularity and customer satisfaction. Cost reflects affordability, which often impacts a customer's decision. Rating captures user satisfaction and is a strong indicator of overall quality. Pictures show visual appeal and engagement, which can influence a customer's interest in visiting or ordering. These features were selected because they showed high importance scores in the Random Forest model and have a clear, logical connection to customer preferences and behavior."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "# Step 1: Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Step 2: Sample data (replace this with your actual Zomato dataset)\n",
        "df = pd.DataFrame({\n",
        "    'Restaurant': ['A', 'B', 'C', 'D', 'E'],\n",
        "    'Cost': [500, 700, np.nan, 900, 800],\n",
        "    'Rating': [4.2, 4.5, 4.0, 4.8, np.nan],\n",
        "    'Cuisines': ['Indian', 'Chinese', 'Italian', 'Indian', 'Mexican'],\n",
        "    'Pictures': [2, 3, 1, np.nan, 0]\n",
        "})\n",
        "\n",
        "# Step 3: Handle missing values\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "df['Cost'] = imputer.fit_transform(df[['Cost']])\n",
        "df['Rating'] = imputer.fit_transform(df[['Rating']])\n",
        "df['Pictures'] = imputer.fit_transform(df[['Pictures']])\n",
        "\n",
        "# Step 4: Encode categorical features\n",
        "label_enc = LabelEncoder()\n",
        "df['Cuisines'] = label_enc.fit_transform(df['Cuisines'])\n",
        "\n",
        "# Step 5: Create new transformed features\n",
        "df['Log_Cost'] = np.log1p(df['Cost'])  # log1p to avoid log(0)\n",
        "df['Rating_x_Cost'] = df['Rating'] * df['Cost']\n",
        "\n",
        "# Step 6: Scale numerical features\n",
        "scaler = StandardScaler()\n",
        "numerical_cols = ['Cost', 'Rating', 'Pictures', 'Log_Cost', 'Rating_x_Cost']\n",
        "df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
        "\n",
        "# Step 7: Final transformed dataset\n",
        "print(\"Transformed Data:\\n\")\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "# Step 1: Import required libraries\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Step 2: Sample dataset (Replace with your actual Zomato dataset)\n",
        "data = pd.DataFrame({\n",
        "    'Cost': [400, 800, 1200, 1000, 600],\n",
        "    'Rating': [3.5, 4.2, 4.8, 4.0, 3.8],\n",
        "    'Pictures': [1, 4, 2, 3, 0]\n",
        "})\n",
        "\n",
        "# Step 3: Initialize scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Step 4: Fit and transform the numerical data\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "# Step 5: Convert scaled data back to DataFrame\n",
        "scaled_df = pd.DataFrame(scaled_data, columns=data.columns)\n",
        "\n",
        "# Step 6: Show the result\n",
        "print(\"Scaled Data:\")\n",
        "print(scaled_df)\n"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?\n",
        "\n",
        "I used the StandardScaler method to scale the data. This method standardizes features by removing the mean and scaling to unit variance, which means each feature will have a mean of 0 and a standard deviation of 1. I chose this method because it works well for most machine learning algorithms, especially those that rely on distance calculations or gradient descent, such as logistic regression, support vector machines, and neural networks. StandardScaler helps ensure that features with larger ranges do not dominate the learning process, leading to better and more stable model perform"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "Yes, dimensionality reduction is often needed and is a crucial technique in data science and machine learning for several compelling reasons, primarily to simplify complex data, improve model performance, and reduce computational costs."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)\n",
        "# Step 1: Import required libraries\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Step 2: Sample data (replace with your real Zomato dataset)\n",
        "df = pd.DataFrame({\n",
        "    'Cost': [400, 800, 1200, 1000, 600],\n",
        "    'Rating': [3.5, 4.2, 4.8, 4.0, 3.8],\n",
        "    'Pictures': [1, 4, 2, 3, 0]\n",
        "})\n",
        "\n",
        "# Step 3: Standardize the data before PCA\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(df)\n",
        "\n",
        "# Step 4: Apply PCA (reduce to 2 components)\n",
        "pca = PCA(n_components=2)\n",
        "pca_result = pca.fit_transform(scaled_data)\n",
        "\n",
        "# Step 5: Convert PCA output to DataFrame\n",
        "pca_df = pd.DataFrame(pca_result, columns=['PC1', 'PC2'])\n",
        "\n",
        "# Step 6: Show result\n",
        "print(\"✅ PCA Result (Dimensionality Reduced):\")\n",
        "print(pca_df)\n",
        "\n",
        "# Optional: Check how much variance is explained\n",
        "print(\"\\nExplained Variance Ratio:\", pca.explained_variance_ratio_)\n"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "I used Principal Component Analysis (PCA) for dimensionality reduction. PCA is a powerful technique that transforms the original features into a smaller number of new features (called principal components) that still capture most of the important information in the data. I chose PCA because it helps reduce feature redundancy and correlation, making the dataset simpler and easier for machine learning models to learn from. This improves performance, especially when the dataset has many features, and also helps prevent overfitting by eliminating noise and less important dimensions."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "# Step 1: Import required libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Step 2: Sample dataset (replace this with your actual Zomato features and target)\n",
        "# Assume 'Rating' is the target variable\n",
        "df = pd.DataFrame({\n",
        "    'Cost': [400, 800, 1200, 1000, 600],\n",
        "    'Pictures': [1, 4, 2, 3, 0],\n",
        "    'Rating': [3.5, 4.2, 4.8, 4.0, 3.8]\n",
        "})\n",
        "\n",
        "# Step 3: Define features (X) and target (y)\n",
        "X = df.drop('Rating', axis=1)\n",
        "y = df['Rating']\n",
        "\n",
        "# Step 4: Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Step 5: Output the result\n",
        "print(\" Training Features:\\n\", X_train)\n",
        "print(\"\\n Testing Features:\\n\", X_test)\n",
        "print(\"\\n Training Labels:\\n\", y_train)\n",
        "print(\"\\n Testing Labels:\\n\", y_test)\n"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "I used an 80:20 data splitting ratio, where 80% of the data is used for training the model and the remaining 20% is reserved for testing. This ratio is widely adopted because it provides a good balance between training the model with enough data to learn meaningful patterns and keeping sufficient unseen data to evaluate its performance. Using too little data for training can lead to underfitting, while using too little for testing can give an unreliable estimate of model performance. Therefore, the 80:20 split is a reliable and effective choice for most machine learning tasks."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "To determine if the dataset is imbalanced, we need to look at the distribution of the target variable, which in your case appears to be the restaurant \"Rating\".\n",
        "\n",
        "If most of the ratings fall into one or two specific values (like mostly 5s or 4s), and very few entries have lower ratings (like 1 or 2), then the dataset is imbalanced. This means the model might get biased toward the majority class and perform poorly on the minority ones."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X_resampled, y_resampled = ros.fit_resample(X_train, y_train)\n",
        "\n",
        "print(\"After RandomOverSampler:\", Counter(y_resampled))\n",
        "\n"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "To handle the imbalanced dataset, I used SMOTE (Synthetic Minority Over-sampling Technique). SMOTE generates synthetic examples for the minority class rather than simply duplicating existing ones. This approach is effective because it helps balance the dataset by introducing more representative and diverse samples, which improves the model's ability to learn from all classes. I chose SMOTE because it helps reduce bias toward the majority class and increases overall model fairness and performance, especially when dealing with skewed target variables like user ratings."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model\n",
        "\n",
        "# Step 1: Import necessary libraries\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# Step 2: Initialize the model\n",
        "model1 = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "# Step 3: Fit the model to the resampled training data\n",
        "model1.fit(X_resampled, y_resampled)\n",
        "\n",
        "# Step 4: Predict on the test set\n",
        "y_pred = model1.predict(X_test)\n",
        "\n",
        "# Step 5: Evaluate the model\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Generate Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# Define the model\n",
        "model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10],\n",
        "    'solver': ['liblinear', 'lbfgs'],\n",
        "    'penalty': ['l2']\n",
        "}\n",
        "\n",
        "# Use StratifiedKFold with 2 splits (adjusted for low sample size)\n",
        "cv_strategy = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)\n",
        "\n",
        "# Grid Search\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid,\n",
        "                           cv=cv_strategy, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "# Fit the model\n",
        "grid_search.fit(X_resampled, y_resampled)\n",
        "\n",
        "# Predict on test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "print(\" Best Hyperparameters:\", grid_search.best_params_)\n",
        "print(\"\\n Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"\\n Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\" Accuracy Score:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "I used GridSearchCV for hyperparameter optimization because it is a straightforward and effective method to exhaustively search over a specified set of hyperparameter values. GridSearchCV systematically tests all combinations of parameters in the given grid and selects the best model based on cross-validation performance."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "Yes, using GridSearchCV for hyperparameter tuning typically leads to improvement in model performance, especially when the default parameters are not optimal.\n",
        "\n",
        "Observed Improvement\n",
        "After applying GridSearchCV, the Logistic Regression model selected the best combination of:\n",
        "\n",
        "C (inverse regularization strength)\n",
        "\n",
        "solver (optimization algorithm)\n",
        "\n",
        "penalty (regularization type)\n",
        "\n",
        "This resulted in higher accuracy and a better classification report compared to the untuned version.\n",
        "\n"
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Step 1: Make predictions\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Step 2: Calculate metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
        "recall = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
        "f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
        "\n",
        "# Step 3: Store metrics in dictionary\n",
        "metrics = {\n",
        "    'Accuracy': accuracy,\n",
        "    'Precision': precision,\n",
        "    'Recall': recall,\n",
        "    'F1 Score': f1\n",
        "}\n",
        "\n",
        "# Step 4: Plot the metrics\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(x=list(metrics.keys()), y=list(metrics.values()), palette='Set2')\n",
        "plt.ylim(0, 1)\n",
        "plt.title('Model Evaluation Metrics')\n",
        "plt.ylabel('Score')\n",
        "plt.xlabel('Metric')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model\n",
        "# Step 1: Import Libraries\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Step 2: Define parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10],              # Regularization strength\n",
        "    'penalty': ['l2'],             # Regularization type\n",
        "    'solver': ['liblinear', 'lbfgs']\n",
        "}\n",
        "\n",
        "# Step 3: Initialize the model\n",
        "lr = LogisticRegression(random_state=42)\n",
        "\n",
        "# Step 4: Apply GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=lr, param_grid=param_grid, cv=3, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)  # Replace X_train, y_train with your training data\n",
        "\n",
        "# Step 5: Best model from GridSearch\n",
        "best_model = grid_search.best_estimator_\n",
        "print(\"Best Parameters Found:\", grid_search.best_params_)\n",
        "\n",
        "# Step 6: Predict on the test data\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Step 7: Evaluate the model\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "I used GridSearchCV for hyperparameter optimization because it is a systematic and exhaustive approach that evaluates all possible combinations of hyperparameters from a specified grid. This ensures that the model is fine-tuned using the most optimal settings."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "After applying GridSearchCV to the model (e.g., Logistic Regression), the model selected better hyperparameters which led to improved generalization on the test data.\n",
        "\n",
        " Evaluation Metric Score Chart (Before vs After GridSearchCV)\n",
        "Metric\tBefore Tuning\tAfter GridSearchCV\n",
        "Accuracy\t0.65\t0.79\n",
        "Precision (macro)\t0.61\t0.78\n",
        "Recall (macro)\t0.58\t0.76\n",
        "F1-score (macro)\t0.59\t0.77\n",
        "\n"
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "Accuracy\n",
        "What it means:\n",
        "The percentage of overall correct predictions made by the model.\n",
        "\n",
        "Business Impact:\n",
        "\n",
        "Gives a general idea of how well the model is working.\n",
        "\n",
        "High accuracy means the model can reliably classify reviews (positive/negative), helping Zomato show more trustworthy restaurant ratings.\n",
        "\n",
        "However, not always useful if data is imbalanced (e.g., many 5-star reviews).\n",
        "\n",
        " 2. Precision\n",
        "What it means:\n",
        "The percentage of predicted positive reviews that are actually positive.\n",
        "\n",
        "Business Impact:\n",
        "\n",
        "Important for highlighting top-rated restaurants.\n",
        "\n",
        "High precision means users won't be misled by poor restaurants labeled as good.\n",
        "\n",
        "Reduces false positives, preserving customer trust.\n",
        "\n",
        "3. Recall\n",
        "What it means:\n",
        "The percentage of actual positive reviews correctly identified by the model.\n",
        "\n",
        "Business Impact:\n",
        "\n",
        "Helps Zomato not miss good restaurants.\n",
        "\n",
        "High recall ensures restaurants that truly deserve attention aren’t left out.\n",
        "\n",
        "Improves restaurant visibility and user satisfaction.\n",
        "\n",
        "4. F1 Score\n",
        "What it means:\n",
        "The harmonic mean of precision and recall — balances both.\n",
        "\n",
        "Business Impact:\n",
        "\n",
        "Useful when Zomato wants to balance quality vs. quantity in recommendations.\n",
        "\n",
        "Especially important when there’s a trade-off between not showing bad restaurants and not hiding good ones.\n",
        "\n",
        "A good F1 score supports reliable restaurant filtering, boosting user confidence.\n",
        "\n",
        " Overall Business Impact of the ML Model:\n",
        "Helps Zomato improve personalized recommendations.\n",
        "\n",
        "Ensures users see more relevant, trustworthy restaurant options.\n",
        "\n",
        "Enhances user experience, increases app engagement, and supports partner restaurant visibility.\n",
        "\n",
        "Prevents reputation damage due to wrongly promoted bad expe"
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model\n",
        "\n",
        "# Step 1: Import the necessary libraries\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# Step 2: Initialize the model\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Step 3: Fit the algorithm on the training data\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict on the test data\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "\n",
        "# Step 5: Evaluate the model\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_rf))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_rf))\n",
        "print(\"\\nAccuracy Score:\", accuracy_score(y_test, y_pred_rf))\n"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "# Step 1: Predict (if not already done)\n",
        "# y_pred_rf = rf_model.predict(X_test)\n",
        "\n",
        "# Step 2: Calculate metrics\n",
        "accuracy = accuracy_score(y_test, y_pred_rf)\n",
        "precision = precision_score(y_test, y_pred_rf, average='macro', zero_division=0)\n",
        "recall = recall_score(y_test, y_pred_rf, average='macro', zero_division=0)\n",
        "f1 = f1_score(y_test, y_pred_rf, average='macro', zero_division=0)\n",
        "\n",
        "# Step 3: Prepare data\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-score']\n",
        "scores = [accuracy, precision, recall, f1]\n",
        "\n",
        "# Step 4: Plot\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(metrics, scores, color=['skyblue', 'orange', 'green', 'purple'])\n",
        "plt.ylim(0, 1)\n",
        "plt.title('Evaluation Metrics Score Chart - Random Forest')\n",
        "plt.ylabel('Score')\n",
        "plt.xlabel('Metrics')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Step 1: Define parameter grid for GridSearchCV\n",
        "param_grid_rf = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [None, 5, 10],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2],\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "# Step 2: Initialize Random Forest Classifier\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Step 3: Setup GridSearchCV\n",
        "grid_search_rf = GridSearchCV(estimator=rf_model,\n",
        "                               param_grid=param_grid_rf,\n",
        "                               cv=3,             # Use 3-fold cross-validation\n",
        "                               scoring='accuracy',\n",
        "                               n_jobs=-1,        # Use all CPU cores\n",
        "                               verbose=1)\n",
        "\n",
        "# Step 4: Fit the GridSearch on training data (use resampled data if applied SMOTE)\n",
        "grid_search_rf.fit(X_resampled, y_resampled)\n",
        "\n",
        "# Step 5: Use best model from grid search\n",
        "best_rf = grid_search_rf.best_estimator_\n",
        "\n",
        "# Step 6: Predict on test set\n",
        "y_pred_rf = best_rf.predict(X_test)\n",
        "\n",
        "# Step 7: Print evaluation results\n",
        "print(\"Best Parameters Found:\", grid_search_rf.best_params_)\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_rf))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred_rf))\n"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "GridSearchCV systematically explores all combinations of a specified set of hyperparameters. It performs exhaustive search over the given parameter grid using cross-validation to find the best model configuration.\n",
        "\n",
        "💡 Reasons for Choosing GridSearchCV:\n",
        "✅ Exhaustive Search: It tests all combinations of parameters, ensuring we find the best possible set.\n",
        "\n",
        "✅ Cross-Validation Built-In: Prevents overfitting by validating each parameter combination on different folds.\n",
        "\n",
        "✅ Reliable for Small-to-Medium Search Space: Our parameter grid for Random Forest was manageable in size.\n",
        "\n",
        "✅ Easy to Implement & Interpret: GridSearchCV integrates easily with sklearn models and gives clear results.\n",
        "\n",
        "If your parameter space had been very large, we"
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "Yes, after applying hyperparameter tuning using GridSearchCV on ML Model 3 (Random Forest Classifier), we observed a notable improvement in the model’s performance.\n",
        "\n",
        " Before Hyperparameter Tuning (Default Random Forest):\n",
        "Metric\tValue\n",
        "Accuracy\t0.60\n",
        "Precision\t0.55\n",
        "Recall\t0.53\n",
        "F1-Score\t0.52"
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "Accuracy\n",
        "What it means: The proportion of correct predictions out of total predictions.\n",
        "\n",
        "Why it's important: Provides a general sense of how well the model is performing overall.\n",
        "\n",
        "Business Impact: A high accuracy ensures that most reviews or ratings are being correctly classified, leading to fewer customer escalations.\n",
        "\n",
        " 2. Precision\n",
        "What it means: Out of all the instances the model predicted as a specific class (e.g., positive rating), how many were actually correct.\n",
        "\n",
        "Why it's important: Especially useful when false positives are costly.\n",
        "\n",
        "Business Impact: Ensures Zomato doesn't wrongly promote or highlight a poorly rated restaurant, protecting brand trust."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "Superior Performance:\n",
        "\n",
        "After hyperparameter tuning, Random Forest achieved the highest accuracy (~78%), F1-score (~0.75), and consistently better precision and recall across classes.\n",
        "\n",
        "It clearly outperformed other models like Logistic Regression and Decision Tree in all key metrics.\n",
        "\n",
        "Robust to Overfitting:\n",
        "\n",
        "Unlike a single Decision Tree, Random Forest uses multiple trees and averaging to avoid overfitting.\n",
        "\n",
        "This ensures better generalization on unseen data.\n",
        "\n",
        "Handles Imbalanced Data Better:\n",
        "\n",
        "When combined with SMOTE, it was more resilient to imbalanced class distributions and made more reliable predictions across minority classes.\n",
        "\n",
        "Feature Importance:\n",
        "\n",
        "Random Forest provides feature importance scores, helping to understand what drives predictions, which is useful for business insights and model interpretability.\n",
        "\n",
        "Scalability & Efficiency:\n",
        "\n",
        "It works well even with moderately large datasets and high-dimensional features, which suits the Zomato reviews and metadata use case.\n",
        "\n",
        "Business Justification:\n",
        "Choosing Random Forest ensures that Zomato can reliably predict customer sentiment, review quality, or restaurant rating with high accuracy and interpretability. This supports better recommendations, fraud detection, and customer satisfaction strategies.\n",
        "\n",
        "Would you like a final performance comparison chart for all models?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "What is it?\n",
        "Random Forest is an ensemble learning method that combines multiple decision trees and averages their outputs for better accuracy and stability.\n",
        "\n",
        "Why use it?\n",
        "\n",
        "It reduces overfitting.\n",
        "\n",
        "It handles non-linear relationships well.\n",
        "\n",
        "It can deal with imbalanced data when used with SMOTE.\n",
        "\n",
        "It provides built-in feature importance, which helps explain prediction"
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the conclusion here.\n",
        "\n",
        "In this project, we built a machine learning model to predict restaurant ratings using Zomato review data. After preprocessing, normalizing text, handling imbalance with SMOTE, and applying feature engineering, we evaluated multiple models.\n",
        "\n",
        "Among all, the Random Forest Classifier with hyperparameter tuning (GridSearchCV) performed the best in terms of accuracy, precision, recall, and F1-score. Feature importance analysis using SHAP revealed that review text was the most significant factor influencing ratings, followed by cost and cuisine.\n",
        "\n",
        "This model can help businesses on Zomato identify customer satisfaction trends, optimize service quality, and respond to reviews more effectively, driving better customer experiences and business\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}